{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314d1f92",
   "metadata": {},
   "source": [
    "### Creating a ReAct, Multimodal and Agentic Multimodal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "174cbcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ReAct (Reasoning + Acting) is a framework where an LLM:\n",
    "Reasons step by step (eg: decomposes questions, makes decisions)\n",
    "Acts by calling tools like search, retrievers or other tools\n",
    "\"\"\"\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import Tool\n",
    "from typing import Annotated, TypedDict, Sequence, List, Dict\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7438d00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000292EE5E7FB0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000292EE5CB0E0>, root_client=<openai.OpenAI object at 0x00000292EE5B3950>, root_async_client=<openai.AsyncOpenAI object at 0x00000292EE5D8E90>, model_name='gpt-5-nano', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39976cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.vectorstores.faiss.FAISS'>\n"
     ]
    }
   ],
   "source": [
    "# loading data and creating retriever\n",
    "\n",
    "def create_retriever(chunk_size: int, chunk_overlap: int):\n",
    "    # loading the data\n",
    "    docs = PyPDFLoader(\"data/AI2Agent.pdf\").load()\n",
    "\n",
    "    # splitting data into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    # initiate vectorstore\n",
    "    embedding = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "    print(type(vectorstore))\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    return retriever\n",
    "\n",
    "retriever = create_retriever(chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da31c5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000292EE5B3410>, search_kwargs={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d977b38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever', description='Use this tool to fetch relevant knowledge base info', func=<function retriever_tool_fn at 0x00000292F06DF420>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the tools\n",
    "def retriever_tool_fn(query: str):\n",
    "    docs = retriever.invoke(query)\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "retriever_tool = Tool(\n",
    "    name=\"retriever\",\n",
    "    description=\"Use this tool to fetch relevant knowledge base info\",\n",
    "    func=retriever_tool_fn\n",
    ")\n",
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe76e437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gsben\\AppData\\Local\\Temp\\ipykernel_41816\\3838766103.py:4: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  react_node = create_react_agent(llm, tools)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000292EE5DA360>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [retriever_tool]\n",
    "\n",
    "# create the ReAct agent\n",
    "react_node = create_react_agent(llm, tools)\n",
    "react_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "780a6d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000292EF6241D0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the graph\n",
    "class state(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(state_schema=state)\n",
    "builder.add_node(\"react_node\", react_node)\n",
    "builder.set_entry_point(\"react_node\")\n",
    "builder.add_edge(\"react_node\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47bb4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final answer: \n",
      " AI2Agent is an end-to-end framework for deploying AI projects as autonomous agents. Its goal is to make AI deployments more automated, reusable, and adaptable by turning a project into a self-contained agent that can execute tasks guided by predefined guidelines.\n",
      "\n",
      "Key ideas:\n",
      "- Automatic transformation: Takes an AI project and converts it into an autonomous agent that can reason, act, and adapt.\n",
      "- Intelligent reasoning and debugging: Integrates reasoning, automated debugging, and iterative experience accumulation to improve deployment success over time.\n",
      "- Guided, automated workflow: When you submit a request (for example, “generate a talk show in a specific style”), AI2Agent searches for a suitable project, follows a predefined execution guide, auto-deploys, and auto-diagnoses issues to ensure success.\n",
      "- Beyond traditional DevOps: Unlike static templates used by DevOps/AutoDevOps, AI2Agent emphasizes autonomous decision-making, dynamic deployment, and iterative refinement.\n",
      "\n",
      "Core components (as described in the paper):\n",
      "- End-to-end framework to deploy AI projects as autonomous agents\n",
      "- Local auto-deployment and agent auto-packaging\n",
      "- Automated debugging and iterative improvement\n",
      "- A user interface that orchestrates search, guidelines, deployment, and debugging\n",
      "\n",
      "Practical takeaway:\n",
      "- It aims to reduce manual setup and enable AI projects to run more autonomously and robustly, with the ability to adapt as needs change.\n",
      "\n",
      "If you’d like, I can summarize specific sections (e.g., method, limitations, ethics) or discuss potential use cases.\n"
     ]
    }
   ],
   "source": [
    "# Run the ReAct agent\n",
    "\n",
    "user_query = \"What is the AI2Agent framework about?\"\n",
    "state = {\"messages\": HumanMessage(content=user_query)}\n",
    "result = graph.invoke(state)\n",
    "print(f\"\\nFinal answer: \\n {result[\"messages\"][-1].content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2a3a7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final answer: \n",
      " Here’s a concise view grounded in the AI2Agent description you shared: it’s an end-to-end framework that automates deploying AI projects as autonomous agents, with features like intelligent reasoning, automated debugging, and iterative experience accumulation. The text also notes that there are limitations and a call for community contributions to improve it.\n",
      "\n",
      "Limitations (areas that typically need attention in such frameworks)\n",
      "- Deployment complexity as projects scale\n",
      "  - As AI projects grow more complex, ensuring seamless, reliable deployments can become harder and may require more sophisticated orchestration and monitoring.\n",
      "- Dependence on underlying AI models and components\n",
      "  - The quality of automation depends on the accuracy and stability of the embedded models and tools; model drift and updates can affect reliability.\n",
      "- Transparency and debugability\n",
      "  - Automated reasoning and debugging can be opaque; there’s a risk that decisions or fixes are hard to interpret or reproduce without strong traceability.\n",
      "- Security, privacy, and governance\n",
      "  - Auto-deployment and agent packaging can raise risks around data leakage, access control, and policy compliance if not carefully managed.\n",
      "- Portability and interoperability\n",
      "  - Integrating with diverse stacks, cloud providers, and tooling can lead to compatibility challenges or vendor lock-in without standard interfaces.\n",
      "- Reproducibility and benchmarking\n",
      "  - Without standardized benchmarks and reproducible experiments, it’s hard to compare deployments or validate improvements across teams.\n",
      "- Resource use and cost\n",
      "  - Automated processes may incur additional compute, storage, or orchestration overhead; efficiency and cost control are important.\n",
      "- Maturity of the ecosystem\n",
      "  - Being described as an evolving framework, it may rely on ongoing community contributions; onboarding, documentation, and governance skills become important.\n",
      "\n",
      "Potential improvements and exploration areas\n",
      "- Strengthen planning, reasoning, and coordination\n",
      "  - Improve multi-step planning, constraint handling, and coordination among multiple agents or services; examples include better task decomposition and robust rollback strategies.\n",
      "- Enhance auto-deployment and lifecycle management\n",
      "  - Add safer deployment techniques (canary, blue/green), more granular rollback, dependency versioning, and environment-agnostic packaging.\n",
      "- Improve automated debugging and reliability\n",
      "  - Develop root-cause analysis, automated patch generation, and stronger test harnesses to catch regressions early.\n",
      "- Expand explainability and auditability\n",
      "  - Provide transparent decision logs, rationale for actions, and easily navigable traces of agent workflows for compliance and debugging.\n",
      "- Elevate security, privacy, and governance\n",
      "  - Integrate access controls, data handling policies, sandboxing, and policy enforcement to reduce risk during automation.\n",
      "- Boost observability and metrics\n",
      "  - Build dashboards and metrics for success rates, failure modes, latency, resource usage, and reproducibility; enable event tracing and alerting.\n",
      "- Improve extensibility and ecosystem\n",
      "  - Create a robust plugin framework or marketplace for adapters, integrations, and domain recipes; simplify blueprint sharing and reuse.\n",
      "- Upgrade UI/UX and onboarding\n",
      "  - Provide guided setup, templates, and wizards to lower the learning curve; improve documentation and examples.\n",
      "- Establish benchmarks and evaluation standards\n",
      "  - Develop standardized tasks, datasets, and evaluation protocols to enable apples-to-apples comparisons and track progress over time.\n",
      "- Enhance interoperability\n",
      "  - Ensure compatibility with popular MLOps stacks, CI/CD pipelines, and cloud providers through standard APIs and adapters.\n",
      "- Performance optimization\n",
      "  - Explore parallelization, caching, and more efficient resource management to reduce latency and cost.\n",
      "- Domain-specific guidance\n",
      "  - Offer task-specific guidelines or “recipes” for common AI deployment scenarios (e.g., NLP assistants, vision systems, data pipelines).\n",
      "- Community and governance\n",
      "  - Encourage open contributions with clear contribution guidelines, tests, and review processes; establish code of conduct and release governance.\n",
      "\n",
      "If you’d like, I can tailor a concrete improvement plan for your context (e.g., your tech stack, team size, security requirements, and typical AI projects), including a prioritized roadmap, concrete milestones, and suggested metrics to track progress.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"What are the limitations and potential improvements to add or explore about the AI2Agent framework?\"\n",
    "state = {\"messages\": HumanMessage(content=user_query)}\n",
    "result = graph.invoke(state)\n",
    "print(f\"\\nFinal answer: \\n {result[\"messages\"][-1].content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c1b2a",
   "metadata": {},
   "source": [
    "### Building a Multimodal RAG that handles (text + images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b091089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.schema import Document\n",
    "import io\n",
    "import base64\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b46ae69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CLIP model and CLIP Processor\n",
    "# Initialize the CLIP model for unified embeddings\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fe2f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multimodal EMbedding class\n",
    "# @dataclass\n",
    "# class MUltiModalEmbeddings:\n",
    "#     \"\"\"Class for creating multimodal embeddings\"\"\"\n",
    "#     def embed_image(self, image_data):\n",
    "#         if isinstance(image_data, str):\n",
    "#             image = Image.open(image_data).convert(\"RGB\")\n",
    "#         else: # if PIL image\n",
    "#             image = image_data\n",
    "        \n",
    "#         inputs = processor(imaes=image, return_tensors=\"pt\")\n",
    "#         with torch.no_grad():\n",
    "#             features = clip_model.get_image_features(**inputs)\n",
    "#             # Normalize the features\n",
    "#             features = self.normalize_features(features)\n",
    "\n",
    "#     def embed_text(self, text):\n",
    "#         \"\"\"Embed text using CLIP\"\"\"\n",
    "#         inputs = processor(\n",
    "#             text=text,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=77 # CLIP's max token\n",
    "#         )\n",
    "#         with torch.no_grad():\n",
    "#             features = clip_model.get_text_features(**inputs)\n",
    "#             features = self.normalize_features(features)\n",
    "\n",
    "\n",
    "#     def normalize_features(self, features):\n",
    "#         features = features / features.norm(dim=-1, keepdim=True)\n",
    "#         return features.squeeze().numpy\n",
    "\n",
    "# class DataEmbedding:\n",
    "#     def __init__(self):\n",
    "#         self.all_docs = []\n",
    "#         self.all_embeddings = []\n",
    "#         self.image_data_store = []\n",
    "\n",
    "#     def loading_data_and_embedding(pdf_path):\n",
    "#         # pdf_path = \"data/AI2Agent.pdf\"\n",
    "#         doc = fitz.open(pdf_path)\n",
    "\n",
    "#         # Text Splitter\n",
    "#         text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "#         for i, page in enumerate(doc):\n",
    "#             # Process text\n",
    "#             text = page.get_text()\n",
    "#             if text.strip():\n",
    "#                 temp_doc = Document(page_content=text, metadata={\"page\":i, \"type\":\"text\"})\n",
    "#                 text_chunks = text_splitter.split_documents([temp_doc])\n",
    "\n",
    "#                 # Embed each chunk using CLIP\n",
    "#                 for chunk in text_chunks:\n",
    "#                     embedding = MUltiModalEmbeddings.embed_text(chunk)\n",
    "#                     self.all_embeddings.append(embedding)\n",
    "#                     self.all_docs.append(chunk)\n",
    "#             # process image\n",
    "#             \"\"\"\n",
    "#             Convert PDF image to PIL image\n",
    "#             Store as Base64 for GPT-4V model\n",
    "#             create CLIP embedding for retrieval\n",
    "#             \"\"\"\n",
    "#             for img_index, img in enumerate(page.get_images(full=True)):\n",
    "#                 try:\n",
    "#                     xref = img[0]\n",
    "#                     base_image = doc.extract_image(xref)\n",
    "#                     image_bytes = base_image[\"image\"]\n",
    "\n",
    "#                     # Convert to PIL Image\n",
    "#                     pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "#                     # create unique identifier\n",
    "#                     image_id = f\"page_{i}_img_{img_index}\"\n",
    "\n",
    "#                     # store image as base64 for later use with GPT-4V\n",
    "#                     buffered = io.BytesIO()\n",
    "#                     pil_image.save(buffered, format=\"PNG\")\n",
    "#                     img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "#                     self.image_data_store[image_id] = img_base64\n",
    "\n",
    "#                     # Embed document using CLIP\n",
    "#                     embedding = MUltiModalEmbeddings.embed_image(pil_image)\n",
    "#                     self.all_embeddings.append(embedding)\n",
    "\n",
    "#                     # create document for image\n",
    "#                     img_doc = Document(page_content=f\"[Image: {image_id}]\", metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id})\n",
    "#                     self.all_docs.append(img_doc)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing image on page {i}, image {img_index}: {e}\")\n",
    "#                     continue\n",
    "#         doc.close()\n",
    "#         return self.all_docs, self.all_embeddings, self.image_data_store\n",
    "    \n",
    "\n",
    "# @dataclass\n",
    "# class VectorStore:\n",
    "#     all_docs: List\n",
    "#     all_embeddings: List\n",
    "#     image_data_store: Dict\n",
    "\n",
    "#     # create unified FAISS vector store with CLIP embeddings\n",
    "#     def create_vectorstore(self):\n",
    "#         embedding_array = np.array(self.all_embeddings)\n",
    "#         # create custom FAISS\n",
    "#         vector_store = FAISS.from_embeddings(\n",
    "#             text_embeddings=[(doc.page_content, emb) for doc, emb in zip(self.all_docs, embedding_array)],\n",
    "#             embedding=None,\n",
    "#             metadatas=[doc.metadata for doc in self.all_docs]\n",
    "#         )\n",
    "#         return vector_store\n",
    "    \n",
    "# @dataclass\n",
    "# class MultiModalRetrieval:\n",
    "#     query: str\n",
    "#     vectorStore: FAISS\n",
    "#     k: int=5\n",
    "\n",
    "#     def retrieve_multimodal(self):\n",
    "#         \"\"\"Unified retrieval for text and images\"\"\"\n",
    "#         # Embed query using CLIP\n",
    "#         query_embedding = MUltiModalEmbeddings.embed_text(self.query)\n",
    "#         # search in unified vector\n",
    "#         results = self.vectorStore.similarity_search_by_vector(\n",
    "#             embedding=query_embedding,\n",
    "#             k=self.k\n",
    "#         )\n",
    "#         return results\n",
    "    \n",
    "#     def create_multimodal_message(self, retrieved_docs, image_data_store):\n",
    "#         \"\"\"Create a message with both text and images for GPT-4V\"\"\"\n",
    "#         content = []\n",
    "\n",
    "#         # Add the query\n",
    "#         content.append({\n",
    "#             \"type\": \"text\",\n",
    "#             \"text\": f\"Question: {self.query}\\n\\nContext:\\n\"\n",
    "#         })\n",
    "\n",
    "#         # Separate text and image documents\n",
    "#         text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "#         image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "\n",
    "#         # Add text context\n",
    "#         if text_docs:\n",
    "#             text_context = \"\\n\\n\".join([\n",
    "#                 f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "#                 for doc in text_docs\n",
    "#             ])\n",
    "#             content.append({\n",
    "#                 \"type\": \"text\",\n",
    "#                 \"text\": f\"Text exerpts:\\n{text_context}\\n\"\n",
    "#             })\n",
    "\n",
    "#         # Add images context\n",
    "#         for doc in image_docs:\n",
    "#             image_id = doc.metadata.get(\"image_id\")\n",
    "#             if image_id and image_id in image_data_store:\n",
    "#                 content.append({\n",
    "#                     \"type\": \"text\",\n",
    "#                     \"text\": f\"\\n[Image from page {doc.metadata['page']}]\\n\"\n",
    "#                 })\n",
    "#                 content.append({\n",
    "#                     \"type\": \"image_url\",\n",
    "#                     \"image_url\": {\n",
    "#                         \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\",\n",
    "#                         \"alt_text\": f\"Image from page {doc.metadata['page']}\"\n",
    "#                     }\n",
    "#                 })\n",
    "\n",
    "#         # Add instruction\n",
    "#         content.append({\n",
    "#             \"type\": \"text\",\n",
    "#             \"text\": \"\\nAnswer the question based on the provided context. If the context does not contain the answer, say 'I don't know'.\"\n",
    "#         })\n",
    "\n",
    "#         return HumanMessage(content=content)\n",
    "    \n",
    "# @dataclass\n",
    "# class MultiModalRAG:\n",
    "#     query: str\n",
    "#     k: int\n",
    "#     llm: ChatOpenAI\n",
    "\n",
    "#     def generate(self):\n",
    "#         \"\"\"Main Pipeline for multimodal RAG\"\"\"\n",
    "#         # retrieve relevant documents\n",
    "#         context_docs = MultiModalRetrieval.retrieve_multimodal(self.query, self.k)\n",
    "\n",
    "#         # create multimodal message\n",
    "#         message = MultiModalRetrieval.create_multimodal_message(self.query, context_docs)\n",
    "\n",
    "#         # Get response from LLM\n",
    "#         response = llm.invoke([message])\n",
    "#         print(f\"\\n Final Answer:\\n {response.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce8b0bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Assume these are initialized somewhere\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "# Multimodal Embedding class\n",
    "@dataclass\n",
    "class MultiModalEmbeddings:  # Fixed: MUltiModalEmbeddings -> MultiModalEmbeddings\n",
    "    \"\"\"Class for creating multimodal embeddings\"\"\"\n",
    "    \n",
    "    def embed_image(self, image_data):\n",
    "        if isinstance(image_data, str):\n",
    "            image = Image.open(image_data).convert(\"RGB\")\n",
    "        else:  # if PIL image\n",
    "            image = image_data\n",
    "        \n",
    "        inputs = processor(images=image, return_tensors=\"pt\")  # Fixed: imaes -> images\n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "            # Normalize the features\n",
    "            features = self.normalize_features(features)\n",
    "            return features  # Fixed: Added return statement\n",
    "\n",
    "    def embed_text(self, text):\n",
    "        \"\"\"Embed text using CLIP\"\"\"\n",
    "        inputs = processor(\n",
    "            text=text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=77  # CLIP's max token\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_text_features(**inputs)\n",
    "            features = self.normalize_features(features)\n",
    "            return features  # Fixed: Added return statement\n",
    "\n",
    "    def normalize_features(self, features):\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()  # Fixed: .numpy -> .numpy()\n",
    "\n",
    "\n",
    "class DataEmbedding:\n",
    "    def __init__(self):\n",
    "        self.all_docs = []\n",
    "        self.all_embeddings = []\n",
    "        self.image_data_store = {}  # Fixed: [] -> {} (should be dict)\n",
    "        self.embedder = MultiModalEmbeddings()  # Fixed: Create instance\n",
    "\n",
    "    def loading_data_and_embedding(self, pdf_path):  # Fixed: Added self parameter\n",
    "        # pdf_path = \"data/AI2Agent.pdf\"\n",
    "        doc = fitz.open(pdf_path)\n",
    "\n",
    "        # Text Splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "        for i, page in enumerate(doc):\n",
    "            # Process text\n",
    "            text = page.get_text()\n",
    "            if text.strip():\n",
    "                temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
    "                text_chunks = text_splitter.split_documents([temp_doc])\n",
    "\n",
    "                # Embed each chunk using CLIP\n",
    "                for chunk in text_chunks:\n",
    "                    embedding = self.embedder.embed_text(chunk.page_content)  # Fixed: Use instance and pass text content\n",
    "                    self.all_embeddings.append(embedding)\n",
    "                    self.all_docs.append(chunk)\n",
    "            \n",
    "            # process image\n",
    "            \"\"\"\n",
    "            Convert PDF image to PIL image\n",
    "            Store as Base64 for GPT-4V model\n",
    "            create CLIP embedding for retrieval\n",
    "            \"\"\"\n",
    "            for img_index, img in enumerate(page.get_images(full=True)):\n",
    "                try:\n",
    "                    xref = img[0]\n",
    "                    base_image = doc.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "\n",
    "                    # Convert to PIL Image\n",
    "                    pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "                    # create unique identifier\n",
    "                    image_id = f\"page_{i}_img_{img_index}\"\n",
    "\n",
    "                    # store image as base64 for later use with GPT-4V\n",
    "                    buffered = io.BytesIO()\n",
    "                    pil_image.save(buffered, format=\"PNG\")\n",
    "                    img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "                    self.image_data_store[image_id] = img_base64\n",
    "\n",
    "                    # Embed document using CLIP\n",
    "                    embedding = self.embedder.embed_image(pil_image)  # Fixed: Use instance\n",
    "                    self.all_embeddings.append(embedding)\n",
    "\n",
    "                    # create document for image\n",
    "                    img_doc = Document(\n",
    "                        page_content=f\"[Image: {image_id}]\",\n",
    "                        metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "                    )\n",
    "                    self.all_docs.append(img_doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image on page {i}, image {img_index}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        doc.close()\n",
    "        return self.all_docs, self.all_embeddings, self.image_data_store\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VectorStore:\n",
    "    all_docs: List\n",
    "    all_embeddings: List\n",
    "    image_data_store: Dict\n",
    "\n",
    "    # create unified FAISS vector store with CLIP embeddings\n",
    "    def create_vectorstore(self):\n",
    "        embedding_array = np.array(self.all_embeddings)\n",
    "        # create custom FAISS\n",
    "        vector_store = FAISS.from_embeddings(\n",
    "            text_embeddings=[(doc.page_content, emb) for doc, emb in zip(self.all_docs, embedding_array)],\n",
    "            embedding=None,\n",
    "            metadatas=[doc.metadata for doc in self.all_docs]\n",
    "        )\n",
    "        return vector_store\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultiModalRetrieval:\n",
    "    query: str\n",
    "    vectorStore: FAISS\n",
    "    image_data_store: Dict  # Fixed: Added missing parameter\n",
    "    k: int = 5  # Fixed: Moved to end (default args must come after non-default)\n",
    "\n",
    "    def __post_init__(self):  # Fixed: Create embedder instance\n",
    "        self.embedder = MultiModalEmbeddings()\n",
    "\n",
    "    def retrieve_multimodal(self):\n",
    "        \"\"\"Unified retrieval for text and images\"\"\"\n",
    "        # Embed query using CLIP\n",
    "        query_embedding = self.embedder.embed_text(self.query)  # Fixed: Use instance\n",
    "        # search in unified vector\n",
    "        results = self.vectorStore.similarity_search_by_vector(\n",
    "            embedding=query_embedding,\n",
    "            k=self.k\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def create_multimodal_message(self, retrieved_docs):  # Fixed: Removed image_data_store param (use self)\n",
    "        \"\"\"Create a message with both text and images for GPT-4V\"\"\"\n",
    "        content = []\n",
    "\n",
    "        # Add the query\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Question: {self.query}\\n\\nContext:\\n\"\n",
    "        })\n",
    "\n",
    "        # Separate text and image documents\n",
    "        text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "        image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "\n",
    "        # Add text context\n",
    "        if text_docs:\n",
    "            text_context = \"\\n\\n\".join([\n",
    "                f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "                for doc in text_docs\n",
    "            ])\n",
    "            content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"Text excerpts:\\n{text_context}\\n\"  # Fixed: exerpts -> excerpts\n",
    "            })\n",
    "\n",
    "        # Add images context\n",
    "        for doc in image_docs:\n",
    "            image_id = doc.metadata.get(\"image_id\")\n",
    "            if image_id and image_id in self.image_data_store:  # Fixed: Use self.image_data_store\n",
    "                content.append({\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"\\n[Image from page {doc.metadata['page']}]\\n\"\n",
    "                })\n",
    "                content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{self.image_data_store[image_id]}\",  # Fixed: Use self\n",
    "                        \"alt_text\": f\"Image from page {doc.metadata['page']}\"\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        # Add instruction\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"\\nAnswer the question based on the provided context. If the context does not contain the answer, say 'I don't know'.\"\n",
    "        })\n",
    "\n",
    "        return HumanMessage(content=content)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultiModalRAG:\n",
    "    query: str\n",
    "    vectorStore: FAISS  # Fixed: Added missing parameter\n",
    "    image_data_store: Dict  # Fixed: Added missing parameter\n",
    "    llm: ChatOpenAI\n",
    "    k: int = 5  # Fixed: Moved to end\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"Main Pipeline for multimodal RAG\"\"\"\n",
    "        # retrieve relevant documents\n",
    "        retriever = MultiModalRetrieval(  # Fixed: Create instance with all required params\n",
    "            query=self.query,\n",
    "            vectorStore=self.vectorStore,\n",
    "            image_data_store=self.image_data_store,\n",
    "            k=self.k\n",
    "        )\n",
    "        context_docs = retriever.retrieve_multimodal()  # Fixed: Call on instance\n",
    "\n",
    "        # create multimodal message\n",
    "        message = retriever.create_multimodal_message(context_docs)  # Fixed: Call on instance with correct params\n",
    "\n",
    "        # Get response from LLM\n",
    "        response = self.llm.invoke([message])  # Fixed: Use self.llm\n",
    "        print(f\"\\nFinal Answer:\\n{response.content}\")  # Fixed: Spacing in f-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec38627f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer:\n",
      "The main findings of the paper are:\n",
      "\n",
      "1. Introduction of AI2Agent, a system designed to enhance the deployment of AI applications by promoting efficiency while ensuring responsible AI practices.\n",
      "2. The system employs a self-adaptive debugging process during execution steps to iteratively search for and execute solutions until successful.\n",
      "3. AI2Agent accumulates cases and solutions over time to improve performance.\n",
      "4. It strictly utilizes publicly available APIs without involving the training or fine-tuning of large language models (LLMs).\n",
      "5. The work includes a demonstration case using Spark-TTS, an efficient LLM-based text-to-speech model.\n",
      "\n",
      "Overall, the paper presents a framework for auto-deploying AI agents that facilitates streamlined, efficient, and responsible AI application deployment.\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion\n",
    "path = \"data/AI2Agent.pdf\"\n",
    "llm_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "query = \"What are the main findings of this paper?\"\n",
    "# Initialize data embedding\n",
    "data_embedder = DataEmbedding()\n",
    "all_docs, all_embeddings, image_data_store = data_embedder.loading_data_and_embedding(path)\n",
    "    \n",
    "# Create vector store\n",
    "vs = VectorStore(all_docs, all_embeddings, image_data_store)\n",
    "vector_store = vs.create_vectorstore()\n",
    "    \n",
    "# Run RAG\n",
    "rag = MultiModalRAG(\n",
    "    query=query,\n",
    "    vectorStore=vector_store,\n",
    "    image_data_store=image_data_store,\n",
    "    llm=llm_model,\n",
    "    k=5\n",
    ")\n",
    "rag.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba282cca",
   "metadata": {},
   "source": [
    "###  Multimodal Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da78cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Any\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import Tool, AgentExecutor, create_react_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Assume these are initialized somewhere\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "# ============= MULTIMODAL RAG COMPONENTS (from previous code) =============\n",
    "\n",
    "@dataclass\n",
    "class MultiModalEmbeddings:\n",
    "    \"\"\"Class for creating multimodal embeddings\"\"\"\n",
    "    \n",
    "    def embed_image(self, image_data):\n",
    "        if isinstance(image_data, str):\n",
    "            image = Image.open(image_data).convert(\"RGB\")\n",
    "        else:\n",
    "            image = image_data\n",
    "        \n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_image_features(**inputs)\n",
    "            features = self.normalize_features(features)\n",
    "            return features\n",
    "\n",
    "    def embed_text(self, text):\n",
    "        \"\"\"Embed text using CLIP\"\"\"\n",
    "        inputs = processor(\n",
    "            text=text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=77\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            features = clip_model.get_text_features(**inputs)\n",
    "            features = self.normalize_features(features)\n",
    "            return features\n",
    "\n",
    "    def normalize_features(self, features):\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "\n",
    "class DataEmbedding:\n",
    "    def __init__(self):\n",
    "        self.all_docs = []\n",
    "        self.all_embeddings = []\n",
    "        self.image_data_store = {}\n",
    "        self.embedder = MultiModalEmbeddings()\n",
    "\n",
    "    def loading_data_and_embedding(self, pdf_path):\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "        for i, page in enumerate(doc):\n",
    "            # Process text\n",
    "            text = page.get_text()\n",
    "            if text.strip():\n",
    "                temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
    "                text_chunks = text_splitter.split_documents([temp_doc])\n",
    "\n",
    "                for chunk in text_chunks:\n",
    "                    embedding = self.embedder.embed_text(chunk.page_content)\n",
    "                    self.all_embeddings.append(embedding)\n",
    "                    self.all_docs.append(chunk)\n",
    "            \n",
    "            # Process images\n",
    "            for img_index, img in enumerate(page.get_images(full=True)):\n",
    "                try:\n",
    "                    xref = img[0]\n",
    "                    base_image = doc.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "\n",
    "                    pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "                    image_id = f\"page_{i}_img_{img_index}\"\n",
    "\n",
    "                    buffered = io.BytesIO()\n",
    "                    pil_image.save(buffered, format=\"PNG\")\n",
    "                    img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "                    self.image_data_store[image_id] = img_base64\n",
    "\n",
    "                    embedding = self.embedder.embed_image(pil_image)\n",
    "                    self.all_embeddings.append(embedding)\n",
    "\n",
    "                    img_doc = Document(\n",
    "                        page_content=f\"[Image: {image_id}]\",\n",
    "                        metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
    "                    )\n",
    "                    self.all_docs.append(img_doc)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image on page {i}, image {img_index}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        doc.close()\n",
    "        return self.all_docs, self.all_embeddings, self.image_data_store\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VectorStore:\n",
    "    all_docs: List\n",
    "    all_embeddings: List\n",
    "    image_data_store: Dict\n",
    "\n",
    "    def create_vectorstore(self):\n",
    "        embedding_array = np.array(self.all_embeddings)\n",
    "        vector_store = FAISS.from_embeddings(\n",
    "            text_embeddings=[(doc.page_content, emb) for doc, emb in zip(self.all_docs, embedding_array)],\n",
    "            embedding=None,\n",
    "            metadatas=[doc.metadata for doc in self.all_docs]\n",
    "        )\n",
    "        return vector_store\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultiModalRetrieval:\n",
    "    query: str\n",
    "    vectorStore: FAISS\n",
    "    image_data_store: Dict\n",
    "    k: int = 5\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.embedder = MultiModalEmbeddings()\n",
    "\n",
    "    def retrieve_multimodal(self):\n",
    "        \"\"\"Unified retrieval for text and images\"\"\"\n",
    "        query_embedding = self.embedder.embed_text(self.query)\n",
    "        results = self.vectorStore.similarity_search_by_vector(\n",
    "            embedding=query_embedding,\n",
    "            k=self.k\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def create_multimodal_message(self, retrieved_docs):\n",
    "        \"\"\"Create a message with both text and images for GPT-4V\"\"\"\n",
    "        content = []\n",
    "\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"Question: {self.query}\\n\\nContext:\\n\"\n",
    "        })\n",
    "\n",
    "        text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "        image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
    "\n",
    "        if text_docs:\n",
    "            text_context = \"\\n\\n\".join([\n",
    "                f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
    "                for doc in text_docs\n",
    "            ])\n",
    "            content.append({\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
    "            })\n",
    "\n",
    "        for doc in image_docs:\n",
    "            image_id = doc.metadata.get(\"image_id\")\n",
    "            if image_id and image_id in self.image_data_store:\n",
    "                content.append({\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"\\n[Image from page {doc.metadata['page']}]\\n\"\n",
    "                })\n",
    "                content.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{self.image_data_store[image_id]}\",\n",
    "                        \"alt_text\": f\"Image from page {doc.metadata['page']}\"\n",
    "                    }\n",
    "                })\n",
    "\n",
    "        content.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": \"\\nAnswer the question based on the provided context. If the context does not contain the answer, say 'I don't know'.\"\n",
    "        })\n",
    "\n",
    "        return HumanMessage(content=content)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultiModalRAG:\n",
    "    query: str\n",
    "    vectorStore: FAISS\n",
    "    image_data_store: Dict\n",
    "    llm: ChatOpenAI\n",
    "    k: int = 5\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"Main Pipeline for multimodal RAG\"\"\"\n",
    "        retriever = MultiModalRetrieval(\n",
    "            query=self.query,\n",
    "            vectorStore=self.vectorStore,\n",
    "            image_data_store=self.image_data_store,\n",
    "            k=self.k\n",
    "        )\n",
    "        context_docs = retriever.retrieve_multimodal()\n",
    "\n",
    "        message = retriever.create_multimodal_message(context_docs)\n",
    "\n",
    "        response = self.llm.invoke([message])\n",
    "        \n",
    "        # Return both response and metadata for the agent\n",
    "        return {\n",
    "            \"answer\": response.content,\n",
    "            \"sources\": [{\"page\": doc.metadata[\"page\"], \"type\": doc.metadata[\"type\"]} \n",
    "                       for doc in context_docs],\n",
    "            \"num_images\": len([doc for doc in context_docs if doc.metadata.get(\"type\") == \"image\"]),\n",
    "            \"num_text_chunks\": len([doc for doc in context_docs if doc.metadata.get(\"type\") == \"text\"])\n",
    "        }\n",
    "\n",
    "\n",
    "# ============= AGENTIC RAG WRAPPER =============\n",
    "\n",
    "class MultiModalRAGSystem:\n",
    "    \"\"\"\n",
    "    Singleton class to manage the multimodal RAG system state.\n",
    "    This ensures we don't rebuild embeddings on every query.\n",
    "    \"\"\"\n",
    "    _instance = None\n",
    "    _initialized = False\n",
    "    \n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(MultiModalRAGSystem, cls).__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def initialize(self, pdf_path: str, vision_llm: ChatOpenAI):\n",
    "        \"\"\"Initialize the RAG system with a PDF document\"\"\"\n",
    "        if self._initialized:\n",
    "            print(\"RAG system already initialized. Skipping...\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Initializing multimodal RAG system with {pdf_path}...\")\n",
    "        \n",
    "        # Load and embed data\n",
    "        data_embedder = DataEmbedding()\n",
    "        self.all_docs, self.all_embeddings, self.image_data_store = \\\n",
    "            data_embedder.loading_data_and_embedding(pdf_path)\n",
    "        \n",
    "        # Create vector store\n",
    "        vs = VectorStore(self.all_docs, self.all_embeddings, self.image_data_store)\n",
    "        self.vectorStore = vs.create_vectorstore()\n",
    "        \n",
    "        # Store vision LLM\n",
    "        self.vision_llm = vision_llm\n",
    "        \n",
    "        self._initialized = True\n",
    "        print(f\"✓ RAG system initialized with {len(self.all_docs)} documents\")\n",
    "    \n",
    "    def query(self, question: str, k: int = 5) -> str:\n",
    "        \"\"\"Query the multimodal RAG system\"\"\"\n",
    "        if not self._initialized:\n",
    "            return \"Error: RAG system not initialized. Please load a document first.\"\n",
    "        \n",
    "        rag = MultiModalRAG(\n",
    "            query=question,\n",
    "            vectorStore=self.vectorStore,\n",
    "            image_data_store=self.image_data_store,\n",
    "            llm=self.vision_llm,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        result = rag.generate()\n",
    "        \n",
    "        # Format response for agent\n",
    "        response = f\"{result['answer']}\\n\\n\"\n",
    "        response += f\"[Retrieved {result['num_text_chunks']} text chunks and {result['num_images']} images from \"\n",
    "        response += f\"pages: {', '.join(set(str(s['page']) for s in result['sources']))}]\"\n",
    "        \n",
    "        return response\n",
    "\n",
    "\n",
    "# ============= AGENT TOOLS =============\n",
    "\n",
    "def create_multimodal_rag_tool(rag_system: MultiModalRAGSystem) -> Tool:\n",
    "    \"\"\"Create a LangChain tool for the multimodal RAG system\"\"\"\n",
    "    \n",
    "    def search_document(query: str) -> str:\n",
    "        \"\"\"\n",
    "        Search the loaded PDF document for information. This tool can understand both \n",
    "        text and images in the document. Use this when the question requires information \n",
    "        from the loaded document.\n",
    "        \n",
    "        Args:\n",
    "            query: The question to search for in the document\n",
    "            \n",
    "        Returns:\n",
    "            Answer based on document content with source citations\n",
    "        \"\"\"\n",
    "        return rag_system.query(query)\n",
    "    \n",
    "    return Tool(\n",
    "        name=\"search_document\",\n",
    "        func=search_document,\n",
    "        description=(\n",
    "            \"Useful for answering questions about the loaded PDF document. \"\n",
    "            \"This tool can find information from both text and images in the document. \"\n",
    "            \"Input should be a clear question about the document content. \"\n",
    "            \"Use this tool when the question asks about specific information, diagrams, \"\n",
    "            \"charts, or any content that might be in the document.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def create_calculator_tool() -> Tool:\n",
    "    \"\"\"Simple calculator tool\"\"\"\n",
    "    \n",
    "    def calculate(expression: str) -> str:\n",
    "        \"\"\"\n",
    "        Perform mathematical calculations. \n",
    "        \n",
    "        Args:\n",
    "            expression: A mathematical expression to evaluate (e.g., \"2 + 2\" or \"10 * 5\")\n",
    "            \n",
    "        Returns:\n",
    "            The result of the calculation\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Safe evaluation of mathematical expressions\n",
    "            result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "            return f\"The result is: {result}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error calculating: {str(e)}\"\n",
    "    \n",
    "    return Tool(\n",
    "        name=\"calculator\",\n",
    "        func=calculate,\n",
    "        description=(\n",
    "            \"Useful for performing mathematical calculations. \"\n",
    "            \"Input should be a valid mathematical expression like '2 + 2' or '10 * 5'.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# ============= REACT AGENT SETUP =============\n",
    "\n",
    "def create_agentic_multimodal_rag(\n",
    "    pdf_path: str,\n",
    "    agent_llm: Optional[ChatOpenAI] = None,\n",
    "    vision_llm: Optional[ChatOpenAI] = None,\n",
    "    include_web_search: bool = True\n",
    ") -> AgentExecutor:\n",
    "    \"\"\"\n",
    "    Create a ReAct agent with multimodal RAG capabilities.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to the PDF document to load\n",
    "        agent_llm: LLM for the agent's reasoning (default: gpt-4)\n",
    "        vision_llm: LLM for processing images (default: gpt-4o)\n",
    "        include_web_search: Whether to include web search tool\n",
    "        \n",
    "    Returns:\n",
    "        AgentExecutor ready to use\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize LLMs\n",
    "    if agent_llm is None:\n",
    "        agent_llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    \n",
    "    if vision_llm is None:\n",
    "        vision_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    # Initialize RAG system\n",
    "    rag_system = MultiModalRAGSystem()\n",
    "    rag_system.initialize(pdf_path, vision_llm)\n",
    "    \n",
    "    # Create tools\n",
    "    tools = [\n",
    "        create_multimodal_rag_tool(rag_system),\n",
    "        create_calculator_tool()\n",
    "    ]\n",
    "    \n",
    "    # Optionally add web search\n",
    "    if include_web_search:\n",
    "        web_search = DuckDuckGoSearchRun()\n",
    "        tools.append(\n",
    "            Tool(\n",
    "                name=\"web_search\",\n",
    "                func=web_search.run,\n",
    "                description=(\n",
    "                    \"Useful for searching the internet for current information, \"\n",
    "                    \"facts not in the document, or recent events. \"\n",
    "                    \"Input should be a search query.\"\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Create ReAct prompt\n",
    "    react_prompt = PromptTemplate.from_template(\"\"\"\n",
    "    Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "    {tools}\n",
    "\n",
    "    Use the following format:\n",
    "\n",
    "    Question: the input question you must answer\n",
    "    Thought: you should always think about what to do\n",
    "    Action: the action to take, should be one of [{tool_names}]\n",
    "    Action Input: the input to the action\n",
    "    Observation: the result of the action\n",
    "    ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "    Thought: I now know the final answer\n",
    "    Final Answer: the final answer to the original input question\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    Question: {input}\n",
    "    Thought: {agent_scratchpad}\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create agent\n",
    "    agent = create_react_agent(\n",
    "        llm=agent_llm,\n",
    "        tools=tools,\n",
    "        prompt=react_prompt\n",
    "    )\n",
    "    \n",
    "    # Create agent executor\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent,\n",
    "        tools=tools,\n",
    "        verbose=True,\n",
    "        max_iterations=10,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "    \n",
    "    return agent_executor\n",
    "\n",
    "\n",
    "# ============= USAGE EXAMPLE =============\n",
    "\n",
    "def main():\n",
    "    \"\"\"Example usage of the agentic multimodal RAG system\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"AGENTIC MULTIMODAL RAG SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create the agent\n",
    "    agent = create_agentic_multimodal_rag(\n",
    "        pdf_path=\"data/AI2Agent.pdf\",\n",
    "        agent_llm=ChatOpenAI(model=\"gpt-4\", temperature=0),\n",
    "        vision_llm=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
    "        include_web_search=True\n",
    "    )\n",
    "    \n",
    "    # Example queries\n",
    "    queries = [\n",
    "        # Simple document query\n",
    "        \"What are the main topics covered in this document?\",\n",
    "        \n",
    "        # Query requiring image understanding\n",
    "        \"Describe any diagrams or flowcharts in the document and explain what they show.\",\n",
    "        \n",
    "        # Multi-step reasoning\n",
    "        \"What is the formula shown in the document? Calculate the result if x=10.\",\n",
    "        \n",
    "        # Combining document + web search\n",
    "        \"Compare the approach described in this document with current best practices in the field.\",\n",
    "        \n",
    "        # Pure calculation\n",
    "        \"If the document mentions 3 different methods and each has 5 steps, how many total steps are there?\"\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"QUERY {i}: {query}\")\n",
    "        print('=' * 80)\n",
    "        \n",
    "        try:\n",
    "            result = agent.invoke({\"input\": query})\n",
    "            print(f\"\\n{'*' * 80}\")\n",
    "            print(\"FINAL ANSWER:\")\n",
    "            print('*' * 80)\n",
    "            print(result[\"output\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# ============= INTERACTIVE MODE =============\n",
    "\n",
    "def interactive_mode():\n",
    "    \"\"\"Run the agent in interactive mode\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"INTERACTIVE AGENTIC MULTIMODAL RAG\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nInitializing system...\")\n",
    "    \n",
    "    # Get PDF path from user\n",
    "    pdf_path = input(\"\\nEnter path to PDF document: \").strip()\n",
    "    \n",
    "    # Create agent\n",
    "    agent = create_agentic_multimodal_rag(\n",
    "        pdf_path=pdf_path,\n",
    "        agent_llm=ChatOpenAI(model=\"gpt-4\", temperature=0),\n",
    "        vision_llm=ChatOpenAI(model=\"gpt-4o\", temperature=0),\n",
    "        include_web_search=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ System ready! Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Your question: \").strip()\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not query:\n",
    "            continue\n",
    "        \n",
    "        print(\"\\nProcessing...\\n\")\n",
    "        \n",
    "        try:\n",
    "            result = agent.invoke({\"input\": query})\n",
    "            print(f\"\\n{'=' * 80}\")\n",
    "            print(\"ANSWER:\")\n",
    "            print('=' * 80)\n",
    "            print(result[\"output\"])\n",
    "            print(\"\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment the mode you want to use:\n",
    "    \n",
    "    # main()  # Run predefined examples\n",
    "    interactive_mode()  # Run in interactive mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lifeforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
